{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNtLJlW4v5VF"
   },
   "source": [
    "## Классификация текстов с использованием предобученных языковых моделей.\n",
    "\n",
    "В данном задании вам предстоит обратиться к задаче классификации текстов и решить ее с использованием предобученной модели BERT."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T20:11:38.534518Z",
     "start_time": "2025-03-25T20:11:38.531022Z"
    }
   },
   "source": [
    "import json\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from accelerate import optimizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline\n",
    "# __________end of block__________"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся к набору данных SST-2. Holdout часть данных (которая понадобится вам для посылки) доступна по ссылке ниже."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:36:25.716192Z",
     "start_time": "2025-03-25T19:36:25.694027Z"
    }
   },
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/24f_yandex_ml_trainings/homeworks/hw04_bert_and_co/texts_holdout.json\n",
    "# __________end of block__________"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T20:52:13.753865Z",
     "start_time": "2025-03-25T20:52:01.962545Z"
    }
   },
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "df = pd.read_csv(\n",
    "    \"https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv\",\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "texts_train = df[0].values[:5000]\n",
    "y_train = df[1].values[:5000]\n",
    "texts_test = df[0].values[5000:]\n",
    "y_test = df[1].values[5000:]\n",
    "with open(\"texts_holdout.json\") as iofile:\n",
    "    texts_holdout = json.load(iofile)\n",
    "# __________end of block__________"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь остальной код предстоит написать вам.\n",
    "\n",
    "Для успешной сдачи на максимальный балл необходимо добиться хотя бы __84.5% accuracy на тестовой части выборки__."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T19:55:40.038874Z",
     "start_time": "2025-03-25T19:55:40.034514Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T20:49:51.369226Z",
     "start_time": "2025-03-25T20:48:01.084155Z"
    }
   },
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "# Функция для токенизации\n",
    "def tokenize_and_prepare(texts, labels, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'].to(device),\n",
    "        'attention_mask': encodings['attention_mask'].to(device),\n",
    "        'labels': torch.tensor(labels, dtype=torch.long).to(device)\n",
    "    }\n",
    "\n",
    "\n",
    "# Подготовка данных\n",
    "train_data = tokenize_and_prepare(texts_train, y_train, tokenizer)\n",
    "\n",
    "# Инициализация модели С ОБЯЗАТЕЛЬНЫМ ПЕРЕНОСОМ НА УСТРОЙСТВО\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2).to(device)\n",
    "\n",
    "# Пример цикла обучения с корректным использованием устройств\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "\n",
    "    # Создаем DataLoader прямо из тензоров\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        train_data['input_ids'],\n",
    "        train_data['attention_mask'],\n",
    "        train_data['labels']\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # Все тензоры уже на device, но для надежности:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сдача взадания в контест\n",
    "Сохраните в словарь `out_dict` вероятности принадлежности к первому (положительному) классу"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T20:54:59.876800Z",
     "start_time": "2025-03-25T20:54:44.445266Z"
    }
   },
   "source": [
    "# test_data = tokenize_and_prepare(texts_test, y_test, tokenizer)\n",
    "\n",
    "# print(list(texts_test))\n",
    "\n",
    "def tokenize_and_prepare(texts, tokenizer=None, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'].to(device),\n",
    "        'attention_mask': encodings['attention_mask'].to(device),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_in_batches(model, texts, tokenizer, batch_size=32):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            predictions.append(probs.cpu())  # Переносим на CPU\n",
    "\n",
    "            # Очищаем память\n",
    "            del inputs, outputs, probs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.cat(predictions)\n",
    "\n",
    "holdout = tokenize_and_prepare(texts_holdout, tokenizer)\n",
    "out_dict = {\n",
    "    'train': [float(x.cpu()) for x in predict_in_batches(model, list(texts_train), tokenizer)[:, 1]],\n",
    "    'test': [float(x.cpu()) for x in predict_in_batches(model, list(texts_test), tokenizer)[:, 1]],\n",
    "    'holdout': [float(x.cpu()) for x in predict_in_batches(model, texts_holdout, tokenizer)[:, 1]]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько `assert`'ов для проверки вашей посылки:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T21:05:09.842294Z",
     "start_time": "2025-03-25T21:05:05.661156Z"
    }
   },
   "source": [
    "assert isinstance(out_dict[\"train\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"train\"][0], float), \"Object must be a list of floats\"\n",
    "assert (\n",
    "        len(out_dict[\"train\"]) == 5000\n",
    "), \"The predicted probas list length does not match the train set size\"\n",
    "\n",
    "assert isinstance(out_dict[\"test\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"test\"][0], float), \"Object must be a list of floats\"\n",
    "assert (\n",
    "        len(out_dict[\"test\"]) == 1920\n",
    "), \"The predicted probas list length does not match the test set size\"\n",
    "\n",
    "assert isinstance(out_dict[\"holdout\"], list), \"Object must be a list of floats\"\n",
    "assert isinstance(out_dict[\"holdout\"][0], float), \"Object must be a list of floats\"\n",
    "assert (len(\n",
    "    out_dict[\"holdout\"]) == 500\n",
    "), \"The predicted probas list length does not match the holdout set size\"\n",
    "y_preds = predict_in_batches(model, list(texts_test), tokenizer)\n"
   ],
   "outputs": [],
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите код ниже для генерации посылки."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T21:05:12.407248Z",
     "start_time": "2025-03-25T21:05:12.394740Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_preds = torch.argmax(y_preds, dim=1).cpu().numpy()\n",
    "\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "FILENAME = \"submission_dict_hw_text_classification_with_bert.json\"\n",
    "\n",
    "with open(FILENAME, \"w\") as iofile:\n",
    "    json.dump(out_dict, iofile)\n",
    "print(f\"File saved to `{FILENAME}`\")\n",
    "# __________end of block__________"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9010416666666666\n",
      "File saved to `submission_dict_hw_text_classification_with_bert.json`\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_hw01_texts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "coach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
