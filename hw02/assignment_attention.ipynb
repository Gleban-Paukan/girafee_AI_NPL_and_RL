{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNtLJlW4v5VF"
   },
   "source": [
    "## Домашнее задание №2\n",
    "\n",
    "В данном задании вам предстоит детально рассмотреть механизм Attention (и реализовать несколько его вариантов)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KzvkbMOJWLOY",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:35.612654Z",
     "start_time": "2025-03-24T18:24:31.234373Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoYa0SSBWLOa"
   },
   "source": [
    "### Реализация Attention\n",
    "\n",
    "В данной задаче вам предстоит реализовать механизм Attention, в частности несколько способов подсчета attention scores. Конечно, в популярных фреймворках данный механизм уже реализован, но для лучшего понимания вам предстаит реализовать его с помощью `numpy`.\n",
    "\n",
    "Ваше задание в данной задаче: реализовать `additive` (аддитивный) и `multiplicative` (мультипликативный) варианты Attention. Для вашего удобства (и для примера) `dot product` attention (основанный на скалярном произведении) уже реализован.\n",
    "\n",
    "Детальное описание данных типов Attention доступно в лекционных слайдах."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W4AdVBlUWLOa",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:35.689392Z",
     "start_time": "2025-03-24T18:24:35.628168Z"
    }
   },
   "source": [
    "decoder_hidden_state = np.array([7, 11, 4]).astype(float)[:, None]\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "plt.pcolormesh(decoder_hidden_state)\n",
    "plt.colorbar()\n",
    "plt.title(\"Decoder state\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Decoder state')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBSzFknBWLOa"
   },
   "source": [
    "#### Dot product attention (пример реализации)\n",
    "Рассмотрим единственное состояние энкодера – вектор с размерностью `(n_hidden, 1)`, где `n_hidden = 3`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5icp7akIWLOa",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:35.858540Z",
     "start_time": "2025-03-24T18:24:35.826182Z"
    }
   },
   "source": [
    "single_encoder_hidden_state = np.array([1, 5, 11]).astype(float)[:, None]\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "plt.pcolormesh(single_encoder_hidden_state)\n",
    "plt.colorbar();"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WphleTvUWLOb"
   },
   "source": [
    "Attention score между данными состояниями энкодера и декодера вычисляются просто как скалярное произведение:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lEhDbTZWWLOb",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:35.879066Z",
     "start_time": "2025-03-24T18:24:35.876150Z"
    }
   },
   "source": [
    "np.dot(decoder_hidden_state.T, single_encoder_hidden_state)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztRxnP_6WLOb"
   },
   "source": [
    "В общем случае состояний энкодера, конечно, несколько. Attention scores вычисляются с каждым из состояний энкодера:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jNvqlgA8WLOb",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:35.930027Z",
     "start_time": "2025-03-24T18:24:35.926197Z"
    }
   },
   "source": [
    "encoder_hidden_states = (\n",
    "    np.array([[1, 5, 11], [7, 4, 1], [8, 12, 2], [-9, 0, 1]]).astype(float).T\n",
    ")\n",
    "\n",
    "encoder_hidden_states"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  7.,  8., -9.],\n",
       "       [ 5.,  4., 12.,  0.],\n",
       "       [11.,  1.,  2.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkKXaYbhWLOb"
   },
   "source": [
    "Тогда для подсчета скалярных произведений между единственным состоянием декодера и всеми состояниями энкодера можно воспользоваться следующей функцией (которая по факту представляет собой просто матричное умножение и приведение типов):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OcMlUd1aWLOb",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:35.960035Z",
     "start_time": "2025-03-24T18:24:35.957033Z"
    }
   },
   "source": [
    "def dot_product_attention_score(decoder_hidden_state, encoder_hidden_states):\n",
    "    \"\"\"\n",
    "    decoder_hidden_state: np.array of shape (n_features, 1)\n",
    "    encoder_hidden_states: np.array of shape (n_features, n_states)\n",
    "\n",
    "    return: np.array of shape (1, n_states)\n",
    "        Array with dot product attention scores\n",
    "    \"\"\"\n",
    "    attention_scores = np.dot(decoder_hidden_state.T, encoder_hidden_states)\n",
    "    return attention_scores"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-WkRwfBTWLOc",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.100024Z",
     "start_time": "2025-03-24T18:24:36.096703Z"
    }
   },
   "source": [
    "dot_product_attention_score(decoder_hidden_state, encoder_hidden_states)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106.,  97., 196., -59.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0noaX_iPWLOc"
   },
   "source": [
    "Для подсчета \"весов\" нам необходим Softmax:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zafrHQTLWLOc",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.310179Z",
     "start_time": "2025-03-24T18:24:36.306646Z"
    }
   },
   "source": [
    "def softmax(vector):\n",
    "    \"\"\"\n",
    "    vector: np.array of shape (n, m)\n",
    "\n",
    "    return: np.array of shape (n, m)\n",
    "        Matrix where softmax is computed for every row independently\n",
    "    \"\"\"\n",
    "    nice_vector = vector - vector.max()\n",
    "    exp_vector = np.exp(nice_vector)\n",
    "    exp_denominator = np.sum(exp_vector, axis=1)[:, np.newaxis]\n",
    "    softmax_ = exp_vector / exp_denominator\n",
    "    return softmax_"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zsI6virlWLOc",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.388514Z",
     "start_time": "2025-03-24T18:24:36.385273Z"
    }
   },
   "source": [
    "weights_vector = softmax(\n",
    "    dot_product_attention_score(decoder_hidden_state, encoder_hidden_states)\n",
    ")\n",
    "\n",
    "weights_vector"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.19401262e-040, 1.01122149e-043, 1.00000000e+000,\n",
       "        1.79848622e-111]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuc6VNdeWLOc"
   },
   "source": [
    "Наконец, воспользуемся данными весами и вычислим итоговый вектор, как и описано для dot product attention."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5S1WIvuGWLOc",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.508199Z",
     "start_time": "2025-03-24T18:24:36.474093Z"
    }
   },
   "source": [
    "attention_vector = weights_vector.dot(encoder_hidden_states.T).T\n",
    "print(attention_vector)\n",
    "\n",
    "plt.figure(figsize=(2, 5))\n",
    "plt.pcolormesh(attention_vector, cmap=\"spring\")\n",
    "plt.colorbar()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.]\n",
      " [12.]\n",
      " [ 2.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x200313ed100>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKkIVEClWLOd"
   },
   "source": [
    "Данный вектор аккумулирует в себе информацию из всех состояний энкодера, взвешенную на основе близости к заданному состоянию декодера.\n",
    "\n",
    "Реализуем все вышеописанные преобразования в единой функции:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MX8q3ktpWLOd",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.567814Z",
     "start_time": "2025-03-24T18:24:36.565004Z"
    }
   },
   "source": [
    "def dot_product_attention(decoder_hidden_state, encoder_hidden_states):\n",
    "    \"\"\"\n",
    "    decoder_hidden_state: np.array of shape (n_features, 1)\n",
    "    encoder_hidden_states: np.array of shape (n_features, n_states)\n",
    "\n",
    "    return: np.array of shape (n_features, 1)\n",
    "        Final attention vector\n",
    "    \"\"\"\n",
    "    softmax_vector = softmax(\n",
    "        dot_product_attention_score(decoder_hidden_state, encoder_hidden_states)\n",
    "    )\n",
    "    attention_vector = softmax_vector.dot(encoder_hidden_states.T).T\n",
    "    return attention_vector"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_m1Idc0bWLOd",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.603446Z",
     "start_time": "2025-03-24T18:24:36.601419Z"
    }
   },
   "source": [
    "assert (\n",
    "        attention_vector\n",
    "        == dot_product_attention(decoder_hidden_state, encoder_hidden_states)\n",
    ").all()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D153kfcqWLOd"
   },
   "source": [
    "#### Multiplicative attention\n",
    "Ваша текущая задача: реализовать multiplicative attention.\n",
    "$$ e_i = \\mathbf{s}^TW_{mult}\\mathbf{h}_i $$\n",
    "\n",
    "Матрица весов `W_mult` задана ниже.\n",
    "Стоит заметить, что multiplicative attention позволяет работать с состояниями энкодера и декодера различных размерностей, поэтому состояния энкодера будут обновлены:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kIBqyQH0WLOd",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.615297Z",
     "start_time": "2025-03-24T18:24:36.612297Z"
    }
   },
   "source": [
    "encoder_hidden_states_complex = (\n",
    "    np.array([[1, 5, 11, 4, -4], [7, 4, 1, 2, 2], [8, 12, 2, 11, 5], [-9, 0, 1, 8, 12]])\n",
    "    .astype(float)\n",
    "    .T\n",
    ")\n",
    "\n",
    "W_mult = np.array(\n",
    "    [\n",
    "        [-0.78, -0.97, -1.09, -1.79, 0.24],\n",
    "        [0.04, -0.27, -0.98, -0.49, 0.52],\n",
    "        [1.08, 0.91, -0.99, 2.04, -0.15],\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UktAjsjUWLOd",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.653253Z",
     "start_time": "2025-03-24T18:24:36.650252Z"
    }
   },
   "source": [
    "def multiplicative_attention_score(decoder_hidden_state, encoder_hidden_states, W_mult):\n",
    "    attention_vector_score = np.dot(decoder_hidden_state.T, W_mult)\n",
    "    attention_vector_score = attention_vector_score.dot(encoder_hidden_states)\n",
    "    return attention_vector_score"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG_HZsVYWLOd"
   },
   "source": [
    "Реализуйте подсчет attention согласно формулам и реализуйте итоговую функцию `multiplicative_attention`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ItZIOOehWLOd",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:27:55.253947Z",
     "start_time": "2025-03-24T18:27:55.238414Z"
    }
   },
   "source": [
    "def multiplicative_attention_score(decoder_hidden_state, encoder_hidden_states, W_mult):\n",
    "    tmp = decoder_hidden_state.T @ W_mult\n",
    "    attention_scores = tmp @ encoder_hidden_states\n",
    "    return attention_scores\n",
    "\n",
    "def multiplicative_attention(decoder_hidden_state, encoder_hidden_states, W_mult):\n",
    "    scores = multiplicative_attention_score(decoder_hidden_state, encoder_hidden_states, W_mult)\n",
    "    attention_weights = softmax(scores)\n",
    "    context_vector = encoder_hidden_states @ attention_weights.T\n",
    "    return context_vector\n",
    "\n",
    "\n",
    "print(decoder_hidden_state.shape, encoder_hidden_states.shape, W_mult.shape)\n",
    "print(multiplicative_attention_score(decoder_hidden_state, encoder_hidden_states, W_mult))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) (3, 4) (3, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 40\u001B[0m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m context_vector\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(decoder_hidden_state\u001B[38;5;241m.\u001B[39mshape, encoder_hidden_states\u001B[38;5;241m.\u001B[39mshape, W_mult\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28mprint\u001B[39m(multiplicative_attention_score(decoder_hidden_state, encoder_hidden_states, W_mult))\n",
      "Cell \u001B[1;32mIn[18], line 17\u001B[0m, in \u001B[0;36mmultiplicative_attention_score\u001B[1;34m(decoder_hidden_state, encoder_hidden_states, W_mult)\u001B[0m\n\u001B[0;32m     13\u001B[0m tmp \u001B[38;5;241m=\u001B[39m decoder_hidden_state\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m W_mult  \u001B[38;5;66;03m# (1, 3)\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# encoder_hidden_states -> (3, 4)\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# => (1, 3) x (3, 4) = (1, 4)\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m tmp \u001B[38;5;241m@\u001B[39m encoder_hidden_states\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m attention_scores\n",
      "\u001B[1;31mValueError\u001B[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 5)"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEqtjLtDWLOe"
   },
   "source": [
    "#### Additive attention\n",
    "Теперь вам предстоит реализовать additive attention.\n",
    "\n",
    "$$ e_i = \\mathbf{v}^T \\text{tanh} (W_{add-enc} \\mathbf{h}_i + W_{add-dec} \\mathbf{s}) $$\n",
    "\n",
    "Матрицы весов `W_add_enc` и `W_add_dec` доступны ниже, как и вектор весов `v_add`. Для вычисления активации можно воспользоваться `np.tanh`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j94VYOVeWLOe",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.995226100Z",
     "start_time": "2025-03-24T10:54:36.788890Z"
    }
   },
   "source": [
    "v_add = np.array([[-0.35, -0.58, 0.07, 1.39, -0.79, -1.78, -0.35]]).T\n",
    "\n",
    "W_add_enc = np.array(\n",
    "    [\n",
    "        [-1.34, -0.1, -0.38, 0.12, -0.34],\n",
    "        [-1.0, 1.28, 0.49, -0.41, -0.32],\n",
    "        [-0.39, -1.38, 1.26, 1.21, 0.15],\n",
    "        [-0.18, 0.04, 1.36, -1.18, -0.53],\n",
    "        [-0.23, 0.96, 1.02, 0.39, -1.26],\n",
    "        [-1.27, 0.89, -0.85, -0.01, -1.19],\n",
    "        [0.46, -0.12, -0.86, -0.93, -0.4],\n",
    "    ]\n",
    ")\n",
    "\n",
    "W_add_dec = np.array(\n",
    "    [\n",
    "        [-1.62, -0.02, -0.39],\n",
    "        [0.43, 0.61, -0.23],\n",
    "        [-1.5, -0.43, -0.91],\n",
    "        [-0.14, 0.03, 0.05],\n",
    "        [0.85, 0.51, 0.63],\n",
    "        [0.39, -0.42, 1.34],\n",
    "        [-0.47, -0.31, -1.34],\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l5hwEjHMWLOe",
    "ExecuteTime": {
     "end_time": "2025-03-24T18:24:36.995226100Z",
     "start_time": "2025-03-24T10:54:36.796707Z"
    }
   },
   "source": [
    "def additive_attention_score(decoder_hidden_state, encoder_hidden_states, v_add, W_add_enc, W_add_dec):\n",
    "    th = np.tanh(W_add_enc @ encoder_hidden_states + W_add_dec @ decoder_hidden_state)\n",
    "    return np.dot(v_add.T, th)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV6JBHioWLOe"
   },
   "source": [
    "Реализуйте подсчет attention согласно формулам и реализуйте итоговую функцию `additive_attention`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isPS4hSZWLOe"
   },
   "outputs": [],
   "source": [
    "def additive_attention(\n",
    "        decoder_hidden_state, encoder_hidden_states, v_add, W_add_enc, W_add_dec\n",
    "):\n",
    "    \"\"\"\n",
    "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
    "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
    "    v_add: np.array of shape (n_features_int, 1)\n",
    "    W_add_enc: np.array of shape (n_features_int, n_features_enc)\n",
    "    W_add_dec: np.array of shape (n_features_int, n_features_dec)\n",
    "\n",
    "    return: np.array of shape (n_features_enc, 1)\n",
    "        Final attention vector\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "    attention_vector = softmax(\n",
    "        additive_attention_score(decoder_hidden_state, encoder_hidden_states, v_add, W_add_enc, W_add_dec)\n",
    "    )\n",
    "    attention_vector = attention_vector.dot(encoder_hidden_states.T)\n",
    "    return attention_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJsCCtCiWLOe"
   },
   "source": [
    "Сдайте функции `multiplicative_attention` и `additive_attention` в контест.\n",
    "\n",
    "Не забудьте про импорт `numpy`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjGuVvb7WLOj"
   },
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_hw01_texts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
